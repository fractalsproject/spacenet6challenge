{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpaceNet6Challenge_Baseline_TrainSAROnly_PyTorch_GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPRpvExjdH8pQb70lEkpg8p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fractalsproject/spacenet6challenge/blob/master/notebooks/SpaceNet6Challenge_Baseline_TrainSAROnly_PyTorch_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuKrbBroRrCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Training config\n",
        "#\n",
        "train = False\n",
        "\n",
        "# If trying to resume a training session, this must be the name of the dated directory\n",
        "resume = None\n",
        "\n",
        "# Set to True if you are debugging Solaris at the backend\n",
        "reinstall_solaris = False\n",
        "\n",
        "#\n",
        "# Test config\n",
        "#\n",
        "test = False\n",
        "\n",
        "# Top level director of the weights to load.  Set to None to use from the training session above.\n",
        "read_model_weights_toplevel_dir = None #'./spacenet6challenge/CosmiQ_SN6_Baseline'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVCZ1zisFRzZ",
        "colab_type": "code",
        "outputId": "94e82109-0bc7-4bc0-d347-16213834ed3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# Clone the spacenet6challenge project\n",
        "!rm -r spacenet6challenge\n",
        "!if [ ! -d \"spacenet6challenge\" ]; then git clone --recursive \"https://github.com/fractalsproject/spacenet6challenge.git\" ; else echo \"spacenet6challenge directory already exists\"; fi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'spacenet6challenge'...\n",
            "remote: Enumerating objects: 184, done.\u001b[K\n",
            "remote: Counting objects: 100% (184/184), done.\u001b[K\n",
            "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
            "remote: Total 503 (delta 110), reused 117 (delta 49), pack-reused 319\u001b[K\n",
            "Receiving objects: 100% (503/503), 130.00 KiB | 355.00 KiB/s, done.\n",
            "Resolving deltas: 100% (242/242), done.\n",
            "Submodule 'CosmiQ_SN6_Baseline' (https://github.com/CosmiQ/CosmiQ_SN6_Baseline/) registered for path 'CosmiQ_SN6_Baseline'\n",
            "Submodule 'solaris' (https://github.com/fractalsproject/solaris.git) registered for path 'solaris'\n",
            "Cloning into '/content/spacenet6challenge/CosmiQ_SN6_Baseline'...\n",
            "remote: Enumerating objects: 323, done.        \n",
            "remote: Total 323 (delta 0), reused 0 (delta 0), pack-reused 323        \n",
            "Receiving objects: 100% (323/323), 81.24 MiB | 11.27 MiB/s, done.\n",
            "Resolving deltas: 100% (187/187), done.\n",
            "Cloning into '/content/spacenet6challenge/solaris'...\n",
            "remote: Enumerating objects: 5, done.        \n",
            "remote: Counting objects: 100% (5/5), done.        \n",
            "remote: Compressing objects: 100% (5/5), done.        \n",
            "remote: Total 3951 (delta 0), reused 1 (delta 0), pack-reused 3946        \n",
            "Receiving objects: 100% (3951/3951), 21.84 MiB | 7.65 MiB/s, done.\n",
            "Resolving deltas: 100% (2490/2490), done.\n",
            "Submodule path 'CosmiQ_SN6_Baseline': checked out 'afe2d88a9e0d41685f5d717a559024e86d2935c3'\n",
            "Submodule path 'solaris': checked out 'a1163bc4d322af7253b2ca220f68d80f6a0189e7'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8K270sGLj3E",
        "colab_type": "code",
        "outputId": "5a4af7c6-7582-4423-f748-2b2ee9c42fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# Check if we need to do a full installation\n",
        "try:\n",
        "  import solaris\n",
        "  from solaris import utils\n",
        "  print(\"Found solaris package.  Assuming a previous installation worked.\")\n",
        "  solaris_ok = True\n",
        "except:\n",
        "  import sys\n",
        "  sys.path.append('/content/spacenet6challenge')\n",
        "  import spacenet6.colab.setup\n",
        "  spacenet6.colab.setup.baseline_prereqs(force=True)\n",
        "  solaris_ok = False"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found solaris package.  Assuming a previous installation worked.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QplMAtPq7FnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Useful for print debugging solaris\n",
        "reinstall_solaris = False\n",
        "if reinstall_solaris:\n",
        "  !pip uninstall solaris\n",
        "  !cd /content/spacenet6challenge/solaris && python setup.py clean\n",
        "  !cd /content/spacenet6challenge/solaris && pip install .\n",
        "  import solaris\n",
        "  import importlib\n",
        "  importlib.reload(solaris)\n",
        "  solaris_ok = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y6XgdnI_bgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO:  Why do we need to do this here...\n",
        "if not solaris_ok:\n",
        "  !pip install shapely\n",
        "  !cd /content/spacenet6challenge/solaris && python setup.py clean\n",
        "  !cd /content/spacenet6challenge/solaris && pip install .\n",
        "  try:\n",
        "    #from solaris import utils\n",
        "    #print(\"Found solaris package.  Assuming installation worked.\")\n",
        "    solaris_ok = True\n",
        "  except:\n",
        "    raise Exception(\"You should restart the kernel and run the notebook again.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Fyi901Oz_n",
        "colab_type": "code",
        "outputId": "cec5226a-a333-499f-d76f-5c1594a3bde3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import solaris\n",
        "dir(solaris)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '__version__',\n",
              " 'bin',\n",
              " 'data',\n",
              " 'eval',\n",
              " 'nets',\n",
              " 'raster',\n",
              " 'tile',\n",
              " 'utils',\n",
              " 'vector']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acUsiBeJMwJ6",
        "colab_type": "code",
        "outputId": "9841c8c4-f0c0-4d12-b8b5-6b8e97d391f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import importlib\n",
        "importlib.reload(solaris)\n",
        "import solaris\n",
        "dir(solaris)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '__version__',\n",
              " 'bin',\n",
              " 'data',\n",
              " 'eval',\n",
              " 'nets',\n",
              " 'raster',\n",
              " 'tile',\n",
              " 'utils',\n",
              " 'vector']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUe1vQuqywVo",
        "colab_type": "code",
        "outputId": "ac0231b2-7b88-4972-e605-30e64967e439",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Check tensorflow \n",
        "import tensorflow\n",
        "print(\"tensorflow version=\",tensorflow.version.VERSION)\n",
        "if (tensorflow.version.VERSION!='1.13.1'):\n",
        "  raise Exception(\"You need to restart the kernel and resume from here.\")\n",
        "\n",
        "# Check torch \n",
        "import torch\n",
        "print( \"torch version=\", torch.version.__version__) \n",
        "if not torch.version.__version__.startswith(\"1.5\"):\n",
        "  raise Exception(\"You need restart the kernel and resume from here.\")\n",
        "if not torch.cuda.is_available():\n",
        "  raise Exception(\"Torch cuda is not available.\")\n",
        "else:\n",
        "  print(\"Torch cuda is available.\")\n",
        "\n",
        "# Check all installation packages are available\n",
        "try:\n",
        "  import solaris\n",
        "  from solaris import utils\n",
        "  import sys\n",
        "  !pip install geopandas>=0.7.0\n",
        "  import geopandas\n",
        "  sys.path.append('/content/spacenet6challenge/CosmiQ_SN6_Baseline')\n",
        "  import baseline\n",
        "  sys.path.append('/content/spacenet6challenge')\n",
        "  from spacenet6.cosmiq import baseline_wrap\n",
        "except:\n",
        "  raise Exception(\"Installation checks failed.\")\n",
        "\n",
        "print(\"Installation checks passed.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow version= 1.13.1\n",
            "torch version= 1.5.0+cu101\n",
            "Torch cuda is available.\n",
            "Installation checks passed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwEv9uuDDAVE",
        "colab_type": "code",
        "outputId": "421610b2-e7b9-4c4c-a46a-bfee7a70fbf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# Mount GCP bucket data\n",
        "import sys\n",
        "sys.path.append('./spacenet6challenge')\n",
        "import spacenet6.colab.bucket\n",
        "spacenet6.colab.bucket.mount(\"spacenet_challenge_data\",force_new_mount=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking for jupyter/ipython environment...\n",
            "Passed.\n",
            "Checking for colab environment...\n",
            "Passed.\n",
            "Trying to authenticate GCP user...\n",
            "Running command: \"echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\".  Please wait...\n",
            "OK.\n",
            "Running command: \"curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - \".  Please wait...\n",
            "OK.\n",
            "Running command: \"apt -qq update\".  Please wait...\n",
            "OK.\n",
            "Running command: \"apt -qq install gcsfuse\".  Please wait...\n",
            "OK.\n",
            "Running command: \"mkdir mountOnColab\".  Please wait...\n",
            "OK.\n",
            "Running command: \"gcsfuse --implicit-dirs spacenet_challenge_data mountOnColab\".  Please wait...\n",
            "OK.\n",
            "Done. Getting folder contents...\n",
            "Running command: \"ls -als /content/mountOnColab\".  Please wait...\n",
            "\n",
            "Mount was successful.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiSxhdrg331e",
        "colab_type": "code",
        "outputId": "c30ef506-90c3-46c8-f901-ffde1191f4d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train on SAR only\n",
        "import os\n",
        "import datetime\n",
        "import solaris as sol\n",
        "import model\n",
        "\n",
        "# validate pretrain path\n",
        "pretrain_path = '/content/mountOnColab/pretrain'\n",
        "if not os.path.exists(pretrain_path):\n",
        "  raise Exception(\"pretrain path does not exist.\")\n",
        "\n",
        "# validate sessions toplevel\n",
        "train_sessions_path = '/content/mountOnColab/train_sessions'\n",
        "if not os.path.exists(train_sessions_path):\n",
        "  raise Exception(\"train sessions path does not exist.\")\n",
        "\n",
        "# validate configs dir\n",
        "configs_path = './spacenet6challenge/configs'\n",
        "if not os.path.exists(configs_path):\n",
        "  raise Exception(\"configs path does not exist.\")\n",
        "\n",
        "# create a dir for this training session\n",
        "dpath = str(datetime.datetime.now()).replace(\":\",\"_\").replace(\" \",\"_\")\n",
        "outpath = '%s/%s' % ( train_sessions_path, dpath )\n",
        "os.makedirs(outpath,exist_ok=True)\n",
        "weightspath = os.path.join(outpath, \"weights\")\n",
        "os.makedirs(weightspath,exist_ok=True)\n",
        "\n",
        "# create train.csv from template\n",
        "f = open( os.path.join(configs_path,'train.csv') )\n",
        "txt = f.read()\n",
        "f.close()\n",
        "newtxt = txt.replace('./root',pretrain_path)\n",
        "traincsv = os.path.join( outpath, \"train.csv\")\n",
        "f = open(traincsv, 'w')\n",
        "f.write(newtxt)\n",
        "f.flush()\n",
        "f.close()\n",
        "\n",
        "# create valid.csv from template\n",
        "f = open(os.path.join(configs_path,'valid.csv') )\n",
        "txt = f.read()\n",
        "f.close()\n",
        "newtxt = txt.replace('./root',pretrain_path)\n",
        "validcsv = os.path.join( outpath, \"valid.csv\")\n",
        "f = open(validcsv, 'w')\n",
        "f.write(newtxt)\n",
        "f.flush()\n",
        "f.close()\n",
        "\n",
        "# create a SAR training yaml from template\n",
        "f = open( os.path.join(configs_path, 'train_sar.yaml') )\n",
        "txt = f.read()\n",
        "f.close()\n",
        "newtxt = txt.replace(\"DATEDIR\",outpath)\n",
        "newtxt = newtxt.replace(\"TRAINCSV\",traincsv)\n",
        "newtxt = newtxt.replace(\"VALIDCSV\",validcsv)\n",
        "newyaml = os.path.join( outpath, \"train_sar.yaml\")\n",
        "f = open(newyaml,'w')\n",
        "f.write(newtxt)\n",
        "f.flush()\n",
        "f.close()\n",
        "\n",
        "print(\"Wrote file->%s\" % newyaml)\n",
        "\n",
        "# create a training config from yaml\n",
        "config = sol.utils.config.parse(newyaml)\n",
        "config['pretrained'] = False\n",
        "\n",
        "sar_dict = {\n",
        "    'model_name': 'unet11',\n",
        "    'weight_path': None,\n",
        "    'weight_url': None,\n",
        "    'arch': model.UNet11\n",
        "}\n",
        "\n",
        "# train\n",
        "trainer = sol.nets.train.Trainer(config, custom_model_dict=sar_dict)\n",
        "trainer.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrote file->/content/mountOnColab/train_sessions/2020-05-06_15_34_04.994493/train_sar.yaml\n",
            "Beginning training epoch 0\n",
            "Train Pytorch CUDA AVAIL\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    loss at batch 0: 2.5997495651245117\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 10: 1.3390947580337524\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 20: 1.109626054763794\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 30: 1.0514026880264282\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 40: 1.003505825996399\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 50: 1.1003497838974\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 60: 1.0126011371612549\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 70: 1.0024032592773438\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 80: 1.0928282737731934\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 90: 0.982096791267395\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 100: 1.0070273876190186\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 110: 1.0137742757797241\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 120: 1.1539943218231201\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 130: 0.9852209091186523\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 140: 1.1262941360473633\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 150: 0.9833542108535767\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 160: 1.0931357145309448\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 170: 1.3069993257522583\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 180: 0.9872773885726929\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 190: 0.9882047772407532\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 200: 0.9825311303138733\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 210: 1.1693775653839111\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 220: 1.0284849405288696\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 230: 1.0486176013946533\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 240: 1.0292171239852905\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 250: 1.0540233850479126\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 260: 0.9791597723960876\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 270: 1.0081171989440918\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 280: 1.00050687789917\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 290: 1.2538948059082031\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 300: 1.0441553592681885\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 310: 1.0906974077224731\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 320: 0.977592945098877\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 330: 0.9984245896339417\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "\n",
            "    Validation loss at epoch 0: 1.159016489982605\n",
            "\n",
            "Beginning training epoch 1\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 0: 0.971275269985199\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 10: 0.9852812886238098\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 20: 1.034583568572998\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 30: 1.0869348049163818\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 40: 0.9973428845405579\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 50: 1.0879724025726318\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 60: 1.0187091827392578\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 70: 1.0270929336547852\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 80: 1.2228269577026367\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 90: 0.9942716956138611\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 100: 1.1277066469192505\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 110: 1.0020067691802979\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 120: 1.0488756895065308\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 130: 0.9852306842803955\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 140: 0.9847908616065979\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 150: 1.3817994594573975\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 160: 0.9931588172912598\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "Train Pytorch CUDA AVAIL\n",
            "    loss at batch 170: 1.0735973119735718\n",
            "Train Pytorch CUDA AVAIL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1SIGjSCSe3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if train:\n",
        "\n",
        "  # Verify all the directories\n",
        "  import os\n",
        "  import datetime\n",
        "\n",
        "  train_path = \"/content/mountOnColab/SAR/train/AOI_11_Rotterdam\"\n",
        "  if not os.path.exists(train_path):\n",
        "    raise Exception(\"Could not find training data\")\n",
        "\n",
        "  pretrain_path = '/content/mountOnColab/pretrain'\n",
        "  if not os.path.exists(pretrain_path):\n",
        "    raise Exception('Could not find pretrain data')\n",
        "\n",
        "  train_sessions_path = '/content/mountOnColab/train_sessions'\n",
        "\n",
        "  # are we trying to resume a training session \n",
        "  if resume: \n",
        "\n",
        "    # Get the last trained model and figure out where to resume...\n",
        "    print(\"Will try to resume training from trainining=>\", resume)\n",
        "    dpath = resume\n",
        "    outpath = '%s/%s' % ( train_sessions_path, dpath )\n",
        "    if not os.path.exists( outpath ):\n",
        "      raise Exception(\"Can't resume because %s does not exist.\" % outpath)\n",
        "    subdirs = os.listdir( os.path.join(outpath, \"weights\"))\n",
        "    print(\"listdir=\", subdirs)\n",
        "    epochs = sorted( [ int( subdir.split(\".\")[0] ) for subdir in subdirs if subdir.split(\".\")[0].isnumeric() ] )\n",
        "    print(\"all epochs=\", epochs)\n",
        "    resume_epoch = int(epochs[-1])\n",
        "    resume_weights = os.path.join(outpath,\"weights/%d.hdf5\" % resume_epoch )\n",
        "    raise Exception(\"Not finished.\")\n",
        "  else:\n",
        "\n",
        "    # Create new dated directory for saved weights...\n",
        "    resume_epoch = None\n",
        "    resume_weights = None\n",
        "    dpath = str(datetime.datetime.now()).replace(\":\",\"_\").replace(\" \",\"_\")\n",
        "    outpath = '%s/%s' % ( train_sessions_path, dpath )\n",
        "\n",
        "  os.makedirs(outpath, exist_ok=True)\n",
        "  print(\"model output directory=\", outpath)\n",
        "\n",
        "  weights_dir = os.path.join(outpath,\"weights\")\n",
        "  os.makedirs( weights_dir, exist_ok=True)\n",
        "  print(\"model weights directory=\", weights_dir)\n",
        "\n",
        "  # Start the training session\n",
        "\n",
        "  cmdargs = [\"--train\",\n",
        "                  \"--sardir\",\"%s/SAR-Intensity\" % train_path,\n",
        "                  \"--opticaldir\",\"%s/PS-RGB\" % train_path,\n",
        "                  \"--labeldir\",\"%s/geojson_buildings\" % train_path,\n",
        "                  \"--rotationfile\",\"%s/SummaryData/SAR_orientations.txt\" % train_path,\n",
        "                             \"--rotationfilelocal\",\"%s/SAR_orientations.txt\" % pretrain_path,\n",
        "                              \"--maskdir\", \"%s/masks\" % pretrain_path,\n",
        "                              \"--sarprocdir\", \"%s/sartrain\" % pretrain_path, \n",
        "                              \"--opticalprocdir\",\"%s/optical\" % pretrain_path,\n",
        "                              \"--traincsv\",\"%s/train.csv\" % pretrain_path,\n",
        "                              \"--validcsv\", \"%s/valid.csv\" % pretrain_path,\n",
        "                              \"--opticaltraincsv\", \"%s/opticaltrain.csv\" % pretrain_path,\n",
        "                              \"--opticalvalidcsv\", \"%s/opticalvalid.csv\" % pretrain_path,\n",
        "                              \"--testcsv\",\"%s/test.csv\" % pretrain_path,\n",
        "                              \"--yamlpath\",\"%s/trainsar.yaml\" % outpath,\n",
        "                              \"--opticalyamlpath\",\"%s/optical.yaml\" % outpath,\n",
        "                              \"--modeldir\",\"%s/weights\" % outpath,\n",
        "                              \"--testprocdir\",\"%s/sartest\" % outpath,\n",
        "                              \"--testoutdir\",\"%s/inference_continuous\" % outpath,\n",
        "                              \"--testbinarydir\",\"%s/inference_binary\" % outpath,\n",
        "                              \"--testvectordir\",\"%s/inference_vectors\" % outpath,\n",
        "                              \"--rotate\",\n",
        "                              \"--transferoptical\",\n",
        "                              \"--mintrainsize\",\"20\",\n",
        "                              \"--mintestsize\",\"80\"]\n",
        "  args = baseline_wrap.parse_args(cmdargs)\n",
        "  #print(args)\n",
        "  #import importlib\n",
        "  #importlib.reload(baseline_wrap)\n",
        "  baseline_wrap.invoke(outpath, \"/content/mountOnColab/pretrain\", args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u01ekuPLoMgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate on test data\n",
        "\n",
        "if test:\n",
        "\n",
        "  #model output directory= /content/mountOnColab/train_sessions/2020-05-03_14_04_02.011290\n",
        "  #model weights directory= /content/mountOnColab/train_sessions/2020-05-03_14_04_02.011290/weights\n",
        "\n",
        "  # Verify all the directories\n",
        "  import os\n",
        "  import datetime\n",
        "  import importlib\n",
        "  import sys\n",
        "  sys.path.append('./spacenet6challenge/CosmiQ_SN6_Baseline')\n",
        "  sys.path.append('./spacenet6challenge/')\n",
        "  from spacenet6.cosmiq import baseline_wrap\n",
        "  import baseline\n",
        "  importlib.reload(baseline)\n",
        "  import importlib\n",
        "  importlib.reload(baseline_wrap)\n",
        "\n",
        "  # where is the raw test SAR data?\n",
        "  test_path = \"/content/mountOnColab/SAR/test_public/AOI_11_Rotterdam\"\n",
        "  if not os.path.exists(test_path):\n",
        "    raise Exception(\"Could not find training data\")\n",
        "\n",
        "  # where is the pretrain_pretest data?\n",
        "  pretrain_pretest_path = '/content/mountOnColab/pretrain'\n",
        "  if not os.path.exists(pretrain_pretest_path):\n",
        "    raise Exception('Could not find pretrain data')\n",
        "\n",
        "  # validate the directory where we load the weights.\n",
        "  if not os.path.exists( os.path.join(read_model_weights_toplevel_dir, \"weights\" ) ):\n",
        "    raise Exception(\"model weights dir does not exist.\")\n",
        "\n",
        "  # where to put output from this test eval session?\n",
        "  test_sessions_path = '/content/mountOnColab/tests_sessions'\n",
        "  dpath = str(datetime.datetime.now()).replace(\":\",\"_\").replace(\" \",\"_\")\n",
        "  outpath = '%s/%s' % ( test_sessions_path, dpath )\n",
        "  os.makedirs( os.path.join( outpath, \"sartest\"), exist_ok=True )\n",
        "  os.makedirs( os.path.join( outpath, \"inference_continuous\"), exist_ok=True )\n",
        "  os.makedirs( os.path.join( outpath, \"inference_binary\"), exist_ok=True )\n",
        "  os.makedirs( os.path.join( outpath, \"inference_vectors\"), exist_ok=True )\n",
        "\n",
        "  cmdargs = [ \"--test\",\n",
        "                  \"--testdir\", \"%s/SAR-Intensity\" % test_path,\n",
        "                              \"--rotationfilelocal\",\"%s/SAR_orientations.txt\" % pretrain_pretest_path,\n",
        "                              \"--maskdir\", \"%s/masks\" % pretrain_pretest_path,\n",
        "                              \"--sarprocdir\", \"%s/sartrain\" % pretrain_pretest_path, \n",
        "                              \"--opticalprocdir\",\"%s/optical\" % pretrain_pretest_path,\n",
        "                              \"--testcsv\",\"%s/test.csv\" % pretrain_pretest_path,\n",
        "                              \"--yamlpath\",\"%s/infer.yaml\" % pretrain_pretest_path,\n",
        "                              \"--modeldir\",\"%s/weights\" % read_model_weights_toplevel_dir,\n",
        "                              \"--testprocdir\",\"%s/sartest\" % outpath,\n",
        "                              \"--testoutdir\",\"%s/inference_continuous\" % outpath,\n",
        "                              \"--testbinarydir\",\"%s/inference_binary\" % outpath,\n",
        "                              \"--testvectordir\",\"%s/inference_vectors\" % outpath,\n",
        "                              \"--rotate\",\n",
        "                              \"--mintestsize\",\"80\"]\n",
        "  args = baseline_wrap.parse_args(cmdargs)\n",
        "  #print(args)\n",
        "  #import importlib\n",
        "  #importlib.reload(baseline_wrap)\n",
        "  print(vars(args))\n",
        "  baseline_wrap.invoke(outpath, \"/content/mountOnColab/pretrain\", args)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}